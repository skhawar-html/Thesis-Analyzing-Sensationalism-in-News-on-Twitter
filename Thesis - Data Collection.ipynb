{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tweepy\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import tweepy\n",
    "import csv\n",
    "pd.options.display.max_colwidth = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entering credentials of app created to access API through Twitter developer account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = 'Ibn8e8xnfGc2nAkUQ4Fdu7Yhb'\n",
    "consumer_secret = 'UMc9s6TKaRoPnFXwNe5v9PA45DPQzgXPawRIIxfYFWJjXcufh6'\n",
    "access_token = '1138693958899511296-NrSy2FV5plByuQdKxUNEAXOzC40LUM'\n",
    "access_token_secret = 'Era9RFy8Q0LAFEerl0i8jrVtOzwKJlN4yrpJvthNqlfvv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authorizing Tweepy to access Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating function to extract tweets with desired twitter account name as argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_tweets(screen_name):\n",
    "    #Twitter only allows access to a users most recent 3240 tweets with this method\n",
    "    \n",
    "    #authorize twitter, initialize tweepy\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    api = tweepy.API(auth)\n",
    "    \n",
    "    #creating a list to hold all the tweepy tweets\n",
    "    alltweets = []  \n",
    "    \n",
    "    #making initial request for most recent tweets (200 is the maximum allowed count)\n",
    "    new_tweets = api.user_timeline(screen_name = screen_name,count=200,tweet_mode='extended',include_rts=True)\n",
    "    \n",
    "    #saving most recent tweets\n",
    "    alltweets.extend(new_tweets)\n",
    "    \n",
    "    #saving the ID of the oldest tweet less one\n",
    "    oldest = alltweets[-1].id - 1\n",
    "    \n",
    "    #keep grabbing tweets until there are no tweets left to grab\n",
    "    while len(new_tweets) > 0:\n",
    "        print(f\"getting tweets before {oldest}\")\n",
    "        \n",
    "        #all subsequent requests use the max_id param to prevent duplicates\n",
    "        new_tweets = api.user_timeline(screen_name = screen_name,count=200,max_id=oldest,tweet_mode='extended')\n",
    "        \n",
    "        #saving most recent tweets\n",
    "        alltweets.extend(new_tweets)\n",
    "        \n",
    "        #updating the ID of the oldest tweet less one\n",
    "        oldest = alltweets[-1].id - 1\n",
    "        \n",
    "        print(f\"...{len(alltweets)} tweets downloaded so far\")\n",
    "    \n",
    "    #transforming tweepy tweets into a 2D array that will populate the csv. The fields requested are: Tweet ID, time of publication,full text, favorite count, retweet count, retweet status, language and media type.\n",
    "    outtweets = [[tweet.id_str, tweet.created_at, tweet.retweeted_status.full_text if tweet.full_text.startswith(\"RT @\") else tweet.full_text, tweet.favorite_count, tweet.retweet_count,\"Yes\" if tweet.full_text.startswith(\"RT @\") else \"No\", tweet.lang, tweet.extended_entities[\"media\"][0][\"type\"] if hasattr(tweet, \"extended_entities\") else \"no media\"] for tweet in alltweets]\n",
    "\n",
    "    \n",
    "    #writing the csv file with names : \"{sub-channel name}_tweets.csv\"\n",
    "    with open(f'{screen_name}_tweets.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"id\",\"created_at\",\"text\",\"favorite_count\",\"retweet_count\",\"is_retweet\",\"language\", \"media_type\"])\n",
    "        writer.writerows(outtweets)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting tweets from BuzzFeed Politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting tweets before 1428001324021469185\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1372986165914128383\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1347721211799891968\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1325127480202698752\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1314501417282011137\n",
      "...1200 tweets downloaded so far\n",
      "getting tweets before 1296450060050366463\n",
      "...1400 tweets downloaded so far\n",
      "getting tweets before 1220169417381691392\n",
      "...1600 tweets downloaded so far\n",
      "getting tweets before 1194776099974086655\n",
      "...1800 tweets downloaded so far\n",
      "getting tweets before 1174750516460441599\n",
      "...2000 tweets downloaded so far\n",
      "getting tweets before 1145484559976300544\n",
      "...2200 tweets downloaded so far\n",
      "getting tweets before 1126427354698358783\n",
      "...2400 tweets downloaded so far\n",
      "getting tweets before 1105231566278520831\n",
      "...2600 tweets downloaded so far\n",
      "getting tweets before 1085995454116986879\n",
      "...2800 tweets downloaded so far\n",
      "getting tweets before 1071061043940491263\n",
      "...3000 tweets downloaded so far\n",
      "getting tweets before 1059893697477656575\n",
      "...3200 tweets downloaded so far\n",
      "getting tweets before 1046901591192686591\n",
      "...3250 tweets downloaded so far\n",
      "getting tweets before 1043924981434834943\n",
      "...3250 tweets downloaded so far\n"
     ]
    }
   ],
   "source": [
    "get_all_tweets('BuzzFeedPol')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting tweets from BuzzFeed Sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting tweets before 694579324880551936\n",
      "...399 tweets downloaded so far\n",
      "getting tweets before 686759746741911553\n",
      "...599 tweets downloaded so far\n",
      "getting tweets before 679316160593190911\n",
      "...798 tweets downloaded so far\n",
      "getting tweets before 674791963493842948\n",
      "...998 tweets downloaded so far\n",
      "getting tweets before 671555505639165951\n",
      "...1196 tweets downloaded so far\n",
      "getting tweets before 667753402995236863\n",
      "...1396 tweets downloaded so far\n",
      "getting tweets before 664839265138974719\n",
      "...1596 tweets downloaded so far\n",
      "getting tweets before 661945288047894527\n",
      "...1794 tweets downloaded so far\n",
      "getting tweets before 659573937936965631\n",
      "...1994 tweets downloaded so far\n",
      "getting tweets before 657667130566049795\n",
      "...2194 tweets downloaded so far\n",
      "getting tweets before 655924655233081344\n",
      "...2394 tweets downloaded so far\n",
      "getting tweets before 653757333638619135\n",
      "...2594 tweets downloaded so far\n",
      "getting tweets before 651923224608030719\n",
      "...2794 tweets downloaded so far\n",
      "getting tweets before 648955061645746175\n",
      "...2994 tweets downloaded so far\n",
      "getting tweets before 644200466193190911\n",
      "...3193 tweets downloaded so far\n",
      "getting tweets before 641433446519324671\n",
      "...3193 tweets downloaded so far\n"
     ]
    }
   ],
   "source": [
    "get_all_tweets('BuzzFeedSports')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting tweets from BuzzFeed Arts & Entertainment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting tweets before 1359236671410237440\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1220017303007432704\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1184507097892958207\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1154786575567544319\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1099856103364415487\n",
      "...1200 tweets downloaded so far\n",
      "getting tweets before 1082107787444809727\n",
      "...1399 tweets downloaded so far\n",
      "getting tweets before 1068548971684143105\n",
      "...1598 tweets downloaded so far\n",
      "getting tweets before 1050468942655950854\n",
      "...1798 tweets downloaded so far\n",
      "getting tweets before 1041853821142556671\n",
      "...1998 tweets downloaded so far\n",
      "getting tweets before 1022537374667079680\n",
      "...2198 tweets downloaded so far\n",
      "getting tweets before 1011563252742942720\n",
      "...2398 tweets downloaded so far\n",
      "getting tweets before 1008035470519832575\n",
      "...2598 tweets downloaded so far\n",
      "getting tweets before 1004494604001992705\n",
      "...2798 tweets downloaded so far\n",
      "getting tweets before 1000082088224112639\n",
      "...2998 tweets downloaded so far\n",
      "getting tweets before 994738378498195455\n",
      "...3198 tweets downloaded so far\n",
      "getting tweets before 987010689125433344\n",
      "...3198 tweets downloaded so far\n"
     ]
    }
   ],
   "source": [
    "get_all_tweets('BuzzFeedEnt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting tweets from Huffington Post Politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting tweets before 1485944844979785727\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1483298213696319490\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1480631721905250306\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1476867118138335242\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1471807928172699649\n",
      "...1200 tweets downloaded so far\n",
      "getting tweets before 1468338520473448451\n",
      "...1400 tweets downloaded so far\n",
      "getting tweets before 1465380254747447298\n",
      "...1600 tweets downloaded so far\n",
      "getting tweets before 1461695957662416908\n",
      "...1800 tweets downloaded so far\n",
      "getting tweets before 1458893900102488072\n",
      "...2000 tweets downloaded so far\n",
      "getting tweets before 1456053295504633860\n",
      "...2200 tweets downloaded so far\n",
      "getting tweets before 1453478800407269375\n",
      "...2400 tweets downloaded so far\n",
      "getting tweets before 1450574077396234240\n",
      "...2600 tweets downloaded so far\n",
      "getting tweets before 1447588355030622220\n",
      "...2800 tweets downloaded so far\n",
      "getting tweets before 1443940514663911426\n",
      "...3000 tweets downloaded so far\n",
      "getting tweets before 1441011086501859329\n",
      "...3200 tweets downloaded so far\n",
      "getting tweets before 1438266515615363081\n",
      "...3249 tweets downloaded so far\n",
      "getting tweets before 1437859709772603397\n",
      "...3249 tweets downloaded so far\n"
     ]
    }
   ],
   "source": [
    "get_all_tweets('HuffPostPol')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting tweets from Huffington Post Sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting tweets before 1141130234227515391\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1075448042785001471\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1017860055574695936\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 966456742619025407\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 959014596106473471\n",
      "...1200 tweets downloaded so far\n",
      "getting tweets before 901684632575455232\n",
      "...1400 tweets downloaded so far\n",
      "getting tweets before 865714388304613375\n",
      "...1600 tweets downloaded so far\n",
      "getting tweets before 858792257499037696\n",
      "...1800 tweets downloaded so far\n",
      "getting tweets before 851761522099597312\n",
      "...2000 tweets downloaded so far\n",
      "getting tweets before 841050215650078723\n",
      "...2200 tweets downloaded so far\n",
      "getting tweets before 831370791526793215\n",
      "...2400 tweets downloaded so far\n",
      "getting tweets before 820818716254437375\n",
      "...2600 tweets downloaded so far\n",
      "getting tweets before 810305551695368191\n",
      "...2800 tweets downloaded so far\n",
      "getting tweets before 794270924488998911\n",
      "...3000 tweets downloaded so far\n",
      "getting tweets before 782422887949070335\n",
      "...3200 tweets downloaded so far\n",
      "getting tweets before 769556278167408639\n",
      "...3200 tweets downloaded so far\n"
     ]
    }
   ],
   "source": [
    "get_all_tweets('HuffPostSports')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting tweets from Huffington Post Life"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting tweets before 1483760094596800514\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1477857367484256256\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1471095329395257347\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1465377079906734091\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1459304904003051520\n",
      "...1200 tweets downloaded so far\n",
      "getting tweets before 1453740078359056388\n",
      "...1400 tweets downloaded so far\n",
      "getting tweets before 1447197001351892994\n",
      "...1600 tweets downloaded so far\n",
      "getting tweets before 1440056856160792578\n",
      "...1800 tweets downloaded so far\n",
      "getting tweets before 1433920002374131714\n",
      "...2000 tweets downloaded so far\n",
      "getting tweets before 1428088938653261830\n",
      "...2200 tweets downloaded so far\n",
      "getting tweets before 1422702391892271114\n",
      "...2400 tweets downloaded so far\n",
      "getting tweets before 1417222325523595265\n",
      "...2600 tweets downloaded so far\n",
      "getting tweets before 1410357146294919177\n",
      "...2800 tweets downloaded so far\n",
      "getting tweets before 1405121837806272518\n",
      "...3000 tweets downloaded so far\n",
      "getting tweets before 1398362844597477376\n",
      "...3200 tweets downloaded so far\n",
      "getting tweets before 1393301482821328903\n",
      "...3250 tweets downloaded so far\n",
      "getting tweets before 1392564200195637248\n",
      "...3250 tweets downloaded so far\n"
     ]
    }
   ],
   "source": [
    "get_all_tweets('HuffPostLife')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting tweets from USA Today Politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting tweets before 1483971304122367999\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1479091434246582276\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1470831723671961611\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1465999383325982720\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1459181710495649835\n",
      "...1200 tweets downloaded so far\n",
      "getting tweets before 1455619191722098688\n",
      "...1400 tweets downloaded so far\n",
      "getting tweets before 1451271085698260995\n",
      "...1600 tweets downloaded so far\n",
      "getting tweets before 1447128038123216895\n",
      "...1800 tweets downloaded so far\n",
      "getting tweets before 1442954312213164038\n",
      "...2000 tweets downloaded so far\n",
      "getting tweets before 1439639216435220484\n",
      "...2200 tweets downloaded so far\n",
      "getting tweets before 1435728198558437376\n",
      "...2400 tweets downloaded so far\n",
      "getting tweets before 1431271120972431360\n",
      "...2600 tweets downloaded so far\n",
      "getting tweets before 1428446725988130820\n",
      "...2800 tweets downloaded so far\n",
      "getting tweets before 1425306767844646915\n",
      "...3000 tweets downloaded so far\n",
      "getting tweets before 1422255934752104447\n",
      "...3200 tweets downloaded so far\n",
      "getting tweets before 1418284774721036298\n",
      "...3250 tweets downloaded so far\n",
      "getting tweets before 1417689585568698369\n",
      "...3250 tweets downloaded so far\n"
     ]
    }
   ],
   "source": [
    "get_all_tweets('USATodayDC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting tweets from USA Today Life"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting tweets before 1480388924732182527\n",
      "...399 tweets downloaded so far\n",
      "getting tweets before 1473871233263493125\n",
      "...599 tweets downloaded so far\n",
      "getting tweets before 1468070419483840517\n",
      "...799 tweets downloaded so far\n",
      "getting tweets before 1462565872787202051\n",
      "...999 tweets downloaded so far\n",
      "getting tweets before 1457112689734475777\n",
      "...1199 tweets downloaded so far\n",
      "getting tweets before 1451563967113928703\n",
      "...1399 tweets downloaded so far\n",
      "getting tweets before 1446890465861713921\n",
      "...1599 tweets downloaded so far\n",
      "getting tweets before 1441499790613778436\n",
      "...1799 tweets downloaded so far\n",
      "getting tweets before 1438310485078781951\n",
      "...1999 tweets downloaded so far\n",
      "getting tweets before 1433422854654099458\n",
      "...2199 tweets downloaded so far\n",
      "getting tweets before 1428054981177794563\n",
      "...2399 tweets downloaded so far\n",
      "getting tweets before 1423306861202268160\n",
      "...2599 tweets downloaded so far\n",
      "getting tweets before 1417838773467893764\n",
      "...2799 tweets downloaded so far\n",
      "getting tweets before 1413301880806068227\n",
      "...2999 tweets downloaded so far\n",
      "getting tweets before 1408136684793860096\n",
      "...3198 tweets downloaded so far\n",
      "getting tweets before 1402994901579563009\n",
      "...3248 tweets downloaded so far\n",
      "getting tweets before 1402108276330754049\n",
      "...3248 tweets downloaded so far\n"
     ]
    }
   ],
   "source": [
    "get_all_tweets('USATodayLife')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting tweets from USA Today Sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting tweets before 1487396355878309889\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1485951334243315711\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1484954515849818121\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1483289925856464897\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1482140725907238911\n",
      "...1200 tweets downloaded so far\n",
      "getting tweets before 1480757078541041664\n",
      "...1400 tweets downloaded so far\n",
      "getting tweets before 1480219062936473613\n",
      "...1600 tweets downloaded so far\n",
      "getting tweets before 1478774544794009605\n",
      "...1800 tweets downloaded so far\n",
      "getting tweets before 1477742522336264204\n",
      "...2000 tweets downloaded so far\n",
      "getting tweets before 1476640483728310284\n",
      "...2200 tweets downloaded so far\n",
      "getting tweets before 1474892525664190467\n",
      "...2400 tweets downloaded so far\n",
      "getting tweets before 1473263229107380226\n",
      "...2600 tweets downloaded so far\n",
      "getting tweets before 1472614323453321216\n",
      "...2800 tweets downloaded so far\n",
      "getting tweets before 1471494072091099154\n",
      "...3000 tweets downloaded so far\n",
      "getting tweets before 1470207437521231880\n",
      "...3200 tweets downloaded so far\n",
      "getting tweets before 1468783889816731647\n",
      "...3250 tweets downloaded so far\n",
      "getting tweets before 1468395739353165830\n",
      "...3250 tweets downloaded so far\n"
     ]
    }
   ],
   "source": [
    "get_all_tweets('USATodaySports')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting tweets from L.A. Times Politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting tweets before 1468692868386758662\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1450220300059217936\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1432704799691218945\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1413226529098772481\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1400467420217761798\n",
      "...1200 tweets downloaded so far\n",
      "getting tweets before 1386983521651236865\n",
      "...1400 tweets downloaded so far\n",
      "getting tweets before 1372669213094117378\n",
      "...1600 tweets downloaded so far\n",
      "getting tweets before 1360302724022083584\n",
      "...1800 tweets downloaded so far\n",
      "getting tweets before 1350427699945332735\n",
      "...2000 tweets downloaded so far\n",
      "getting tweets before 1338936938666004480\n",
      "...2200 tweets downloaded so far\n",
      "getting tweets before 1325804084780208127\n",
      "...2400 tweets downloaded so far\n",
      "getting tweets before 1318251571826860032\n",
      "...2600 tweets downloaded so far\n",
      "getting tweets before 1309237170893455359\n",
      "...2800 tweets downloaded so far\n",
      "getting tweets before 1298365534623404031\n",
      "...3000 tweets downloaded so far\n",
      "getting tweets before 1291012083933483012\n",
      "...3200 tweets downloaded so far\n",
      "getting tweets before 1277074316077694975\n",
      "...3250 tweets downloaded so far\n",
      "getting tweets before 1274386675452567551\n",
      "...3250 tweets downloaded so far\n"
     ]
    }
   ],
   "source": [
    "get_all_tweets('latimespolitics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting tweets from L.A. Times Sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting tweets before 1487853120344498175\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1486504844907012095\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1485396665271947263\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1484039705654038529\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1482598245050335232\n",
      "...1200 tweets downloaded so far\n",
      "getting tweets before 1481099880609333248\n",
      "...1400 tweets downloaded so far\n",
      "getting tweets before 1479622215859830783\n",
      "...1600 tweets downloaded so far\n",
      "getting tweets before 1478406208927064064\n",
      "...1800 tweets downloaded so far\n",
      "getting tweets before 1477153839018962943\n",
      "...2000 tweets downloaded so far\n",
      "getting tweets before 1475676707948429314\n",
      "...2200 tweets downloaded so far\n",
      "getting tweets before 1474818482185375746\n",
      "...2400 tweets downloaded so far\n",
      "getting tweets before 1473329160785313792\n",
      "...2600 tweets downloaded so far\n",
      "getting tweets before 1471920562297466884\n",
      "...2800 tweets downloaded so far\n",
      "getting tweets before 1470594056715595782\n",
      "...3000 tweets downloaded so far\n",
      "getting tweets before 1469322267184275455\n",
      "...3200 tweets downloaded so far\n",
      "getting tweets before 1467919956679352323\n",
      "...3250 tweets downloaded so far\n",
      "getting tweets before 1467620913029939202\n",
      "...3250 tweets downloaded so far\n"
     ]
    }
   ],
   "source": [
    "get_all_tweets('latimessports')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting tweets from L.A. Times Entertainment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting tweets before 1484527958417154050\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1480556791268261897\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1474139507108851711\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1470062643113336839\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1466582209498947586\n",
      "...1200 tweets downloaded so far\n",
      "getting tweets before 1463509464343056386\n",
      "...1400 tweets downloaded so far\n",
      "getting tweets before 1460746257425735680\n",
      "...1600 tweets downloaded so far\n",
      "getting tweets before 1457816954274816000\n",
      "...1800 tweets downloaded so far\n",
      "getting tweets before 1455174541701959680\n",
      "...2000 tweets downloaded so far\n",
      "getting tweets before 1451897976843034626\n",
      "...2200 tweets downloaded so far\n",
      "getting tweets before 1449059287251267589\n",
      "...2400 tweets downloaded so far\n",
      "getting tweets before 1446235639490482180\n",
      "...2600 tweets downloaded so far\n",
      "getting tweets before 1443381837963874305\n",
      "...2800 tweets downloaded so far\n",
      "getting tweets before 1440724324558651401\n",
      "...3000 tweets downloaded so far\n",
      "getting tweets before 1438210259873640450\n",
      "...3200 tweets downloaded so far\n",
      "getting tweets before 1435365575941754879\n",
      "...3250 tweets downloaded so far\n",
      "getting tweets before 1434344790825574400\n",
      "...3250 tweets downloaded so far\n"
     ]
    }
   ],
   "source": [
    "get_all_tweets('latimesent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
